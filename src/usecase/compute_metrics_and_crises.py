"""
This script is used to compute hrvanalysis features on RR intervals.

copyright (c) 2021 association aura
spdx-license-identifier: gpl-3.0
"""
import argparse
from collections import defaultdict
import os
import glob
import sys
import re
from typing import Any, Dict, Hashable, Iterable, Iterator, List, Optional, Tuple, Generic, TypeVar

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import shap


sys.path.append('.')
from src.usecase.utilities import convert_args_to_dict


T = TypeVar('T')
Interval = Tuple[int, int]

MIN_PREDICTION_OVERLAP_PERCENT = .75
MAX_OVERPREDICTION = 60


class Peekable(Generic[T]):
    """Enables peeking an iterator without consuming items.

    This class wraps an iterator and allows users to both
    iterate through items (using the `Peekable.next` method)
    and peek items without consuming them (using the `Peekable.peek`
    method).
    """
    EMPTY = object()

    def __init__(self, iterator: Iterator[T]):
        self.it = iterator
        self.value = self.EMPTY

    def peek(self) -> T:
        """Returns the current item without consuming it.

        The returned item will still be returned on future
        `peek()` and `next()` calls.

        Raises
        ------
        If the iterator is empty (`has_next() == False`),
        raises `StopIteration`
        """
        if self.value is self.EMPTY:
            self.value = next(self.it)
        return self.value  # type: ignore

    def next(self) -> T:
        """Returns and consumes the current item.

        The returned item will no longer be returned on future
        `peek()` and `next()` calls.

        Raises
        ------
        If the iterator is empty (`has_next() == False`),
        raises `StopIteration`
        """
        val = self.peek()
        self.value = self.EMPTY
        return val

    def has_next(self) -> bool:
        """Checks whether the underlying iterator has any items left"""
        try:
            self.peek()
            return True
        except StopIteration:
            return False


def compute_metrics_and_crises(cons_folder: str = 'output/preds-v0_6',
                               output_folder: str = 'output/crises-v0_6',
                               loaded_models: Optional[Dict[str, Any]] = None) -> None:
    """Computes a model's metrics and detects seizures

    Parameters
    ----------
    cons_folder : str, optional
        The path of the folder which contains prediction (csv) files.
        The files should be in the format generated by the generate_predictions.py
        script.
        Defaults to 'output/preds-v0_6'.
    output_folder : str, optional
        The path of the folder to which the metrics and seizure information
        should be saved
        Defaults to 'output/crises-v0_6'.
    loaded_models : Optional[Dict[str, Any]], optional
        A dictionary of models used to compute additional metrics.
        The dictionary key is the model name, as given in the prediction files
        in cons_folder.
        Defaults to None.

    This function generates the following files:
    - crises.csv:
        A list of seizures, indicating the patient_id, session_id,
        start/end of seizure and whether this is a real seizure
        or a seizure predicted by a model. If this was a predicted seizure,
        an additional column is added with the name of the model which
        generated the prediction.

    - intersections.csv:
        A list of intersecting real/predicted seizures.
        Each row of the file indicates the real and predicted seizure
        ids (the index of the seizure in the list of real/predicted seizures),
        the duration of intersection (in number of lines of the input dataset)
        and the model which generated the intersection.

    - sessions.csv
        A list of prediction files, along with patient and sessions ids,
        and start and end times.

    - metrics.csv / session_metrics.csv
        A list of metrics for, respectively, the whole dataset and a single
        session. If the loaded_models are provided, the additional metric
        "shap_expected_value" is computed.
    """
    os.makedirs(output_folder, exist_ok=True)
    real_crises = []
    pred_crises = []
    sessions = []
    crises_intersections = []
    global_metrics = defaultdict(lambda: {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0})
    session_metrics = defaultdict(lambda: {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0})
    session_durations = {}
    models = set()
    for i, cons_file in enumerate(glob.iglob(os.path.join(cons_folder, "**/*.csv"), recursive=True)):
        cons_file_name = os.path.basename(cons_file)
        patient_and_session_id = re.fullmatch(r'cons_(\d+_s\d+_t\d+)(?:_t\d+)?.csv', cons_file_name)
        if patient_and_session_id is None:
            raise ValueError("Invalid file name: " + cons_file_name)
        patient_and_session_id = patient_and_session_id.group(1)
        patient_id, session_id = patient_and_session_id.split('_', 1)

        df = pd.read_csv(cons_file, parse_dates=['timestamp'])
        session_durations[(patient_id, session_id)] = df.shape[0]
        df['timestamp'] = df['timestamp'].values.astype(np.int64) / 10**9
        if 'predicted_label' not in df:
            df['predicted_label'] = np.nan
        df.sort_values('interval_start_time')

        sessions.append({
            'patient_id': patient_id,
            'session_id': session_id,
            'cons_file': cons_file,
            'time_start': df.iloc[0]['timestamp'],
            'time_end': df.iloc[-1]['timestamp'],
        })

        def get_crisis(start: int, end: int, model: Optional[str] = None) -> dict:
            return {
                'patient_id': patient_id,
                'session_id': session_id,
                'start': start,
                'end': end,
                'time_start': df.iloc[start]['timestamp'],
                'time_end': df.iloc[end]['timestamp'],
                'type': 'real' if model is None else 'predicted',
                'model': model,
            }

        models.update(df['model'].unique())
        real_ranges = None
        for model in models:
            df_model = df[df['model'] == model]

            df_preds = df_model.dropna(subset=['label', 'predicted_label'])
            if not df_preds.empty:
                tn, fp, fn, tp = confusion_matrix(df_preds['label'], df_preds['predicted_label'], labels=[0, 1]).ravel()
                session_metrics[(patient_id, session_id)] = {
                    'tn': tn,
                    'fp': fp,
                    'fn': fn,
                    'tp': tp,
                }
            global_metrics.setdefault(model, defaultdict(int))
            for metric, val in session_metrics[(patient_id, session_id)].items():
                global_metrics[model][metric] += val

            update_with_derived_metrics(session_metrics[(patient_id, session_id)])

            # Initilize real_ranges only for the first model
            # Other models should use the same data, so we can just reuse the same real_ranges
            if real_ranges is None:
                real_ranges = list(get_ranges(df_model['label'], target_val=1))
            real_it = Peekable(iter(real_ranges))
            pred_it = Peekable(iter(get_ranges(df_model['predicted_label'], target_val=1)))
            for real_interval, pred_interval in split_and_zip_crises(real_it, pred_it):
                if real_interval is not None and not is_last_crisis(real_interval, real_crises):
                    # if the real crisis is not already in the list, append it
                    real_crises.append(get_crisis(*real_interval))
                if pred_interval is not None and not is_last_crisis(pred_interval, pred_crises):
                    # if the predicted crisis is not already in the list, append it
                    pred_crises.append(get_crisis(*pred_interval, model=model))

                # detect intersections
                if real_interval is None or pred_interval is None:
                    continue
                real_start, real_end = real_interval
                pred_start, pred_end = pred_interval
                intersection_start = max(real_start, pred_start)
                intersection_end = min(real_end, pred_end)
                intersection_duration = intersection_end - intersection_start
                if intersection_duration >= 1:
                    crises_intersections.append({
                        'real_id': len(real_crises)-1,
                        'pred_id': len(pred_crises)-1,
                        'duration_elements': intersection_duration,
                        'model': model,
                    })
        print(f'[{i}] Processed {cons_file} - total stats: {len(real_crises)} real crises, {len(pred_crises)} predicted crises')

    tagged_crises = []
    for model in models:
        update_with_derived_metrics(global_metrics[model])
        tagged_crises_for_model, crises_metrics_for_model = \
            compute_metrics(real_crises, pred_crises, crises_intersections, session_durations, model=model)
        global_metrics[model].update(crises_metrics_for_model)
        tagged_crises += tagged_crises_for_model

        if loaded_models is not None and model in loaded_models:
            explainer = shap.TreeExplainer(loaded_models[model])
            global_metrics[model]['shap_expected_value'] = explainer.expected_value[1]

    # We specify manually specify the columns for two reasons:
    # 1) in order to drop some columns before saving the data
    # 2) in order to ensure the column names are present even if there is no data
    pd.DataFrame(real_crises + pred_crises,
                 columns=['patient_id', 'session_id', 'time_start', 'time_end', 'type', 'model']) \
        .to_csv(os.path.join(output_folder, 'crises.csv'), index=False)

    pd.DataFrame(crises_intersections,
                 columns=['real_id', 'pred_id', 'duration_elements', 'model']) \
        .to_csv(os.path.join(output_folder, 'intersections.csv'), index=False)

    pd.DataFrame(sessions,
                 columns=['patient_id', 'session_id', 'cons_file', 'time_start', 'time_end']) \
        .to_csv(os.path.join(output_folder, 'sessions.csv'), index=False)

    pd.DataFrame(tagged_crises,
                 columns=['real_id', 'pred_id', 'time_start', 'time_end', 'patient_id', 'session_id', 'overlap', 'classification', 'model']) \
        .to_csv(os.path.join(output_folder, 'tagged_crises.csv'), index=False)

    pd.DataFrame([{'model': model, 'metric': metric, 'value': value}
                  for model, model_metrics in global_metrics.items()
                  for metric, value in model_metrics.items()]) \
        .to_csv(os.path.join(output_folder, 'metrics.csv'), index=False)

    pd.DataFrame([{'patient_id': patient_id,
                   'session_id': session_id,
                   'metric': metric,
                   'value': value}
                  for (patient_id, session_id), metrics in session_metrics.items()
                  for metric, value in metrics.items()]) \
        .to_csv(os.path.join(output_folder, 'session_metrics.csv'), index=False)


def update_with_derived_metrics(metrics: dict) -> None:
    """Computes metrics derived from the confusion matrix

    The following metrics are computed:
     - specitivity
     - precision
     - recall
     - f1
    These metrics may have value NaN if their computation
    would require a division by zero (e.g. precision is NaN
    if there are no positive predictions).

    Parameters
    ----------
    metrics : dict
        A dictionary which should contain the keys tp, tn, fn and fp.
    """
    safe_div = lambda x, y: x/y if y != 0 else np.nan
    tp = metrics['tp']
    tn = metrics['tn']
    fn = metrics['fn']
    fp = metrics['fp']
    metrics['specitivity'] = safe_div(tn, tn + fp)
    metrics['precision'] = safe_div(tp, tp + fp)
    metrics['recall'] = safe_div(tp, tp + fn) # i.e. sensitivity
    metrics['f1'] = 2 * safe_div(metrics['precision'] * metrics['recall'],
                                 metrics['precision'] + metrics['recall'])


def is_last_crisis(crisis: Interval, crises_list: List[dict]) -> bool:
    if not crises_list:
        return False
    last = crises_list[-1]
    start, end = crisis
    return last['start'] == start and last['end'] == end


def split_and_zip_crises(real_it: Peekable[Interval], pred_it: Peekable[Interval]) \
        -> Iterable[Tuple[Optional[Interval], Optional[Interval]]]:
    """Zips intersecting seizures, splitting them if required.

    Parameters
    ----------
    real_it : Peekable[Interval]
        An iterator of real seizures, in the form of Intervals ((start, end) tuples)

    pred_it : Peekable[Interval]
        An iterator of real seizures, in the form of Intervals ((start, end) tuples)

    Returns
    -------
    An iterator of tuples (real, pred) indicating which real seizures
    happened simultaneously with which predicted seizures.

    A real seizure may be returned more than once if it was predicted
    multiple times.

    A prediction which partially intersects with a real seizure will
    be split into multiple predictions: those which intersect the
    real seizure and those which do not.
    """
    if not real_it.has_next():
        return
    elif not pred_it.has_next():
        # If there are no predicted crisis, just yield the real ones
        while real_it.has_next():
            yield real_it.next(), None
        return

    # These variables will be properly initialized in the first iteration
    # of the while loop. We initialize them now to indicate to linters
    # that these variables are not unbound.
    real_start = real_end = pred_start = pred_end = -1

    # indicates whether we need to fetch a new real/predicted crisis
    update_real = update_pred = True
    while True:
        if update_real:
            if real_it.has_next():
                # If there is another real crisis, get it
                real_start, real_end = real_it.next()
            else:
                # Otherwise, yield the predicted ones and return
                if not update_pred:
                    yield None, (pred_start, pred_end)
                while pred_it.has_next():
                    yield None, pred_it.next()
                return

        if update_pred:
            if pred_it.has_next():
                # If there is another predicted crisis, get it
                pred_start, pred_end = pred_it.next()
            else:
                # Otherwise, yield the real ones and return
                yield (real_start, real_end), None
                while real_it.has_next():
                    yield real_it.next(), None
                return

        # If we're here, then there are both real and predicted crises
        update_real = update_pred = False
        if real_end <= pred_start:
            # the real crisis happens before the predicted one (no overlap)
            yield (real_start, real_end), None
            update_real = True
        elif pred_end <= real_start:
            # the predicted crisis happens before the real one (no overlap)
            yield None, (pred_start, pred_end)
            update_pred = True
        elif real_start - pred_start > MAX_OVERPREDICTION:
            # otherwise, there is overlap

            # the prediction starts too early, so we split it into two parts:
            # - the part of the prediction that's too early (yielded now)
            # - the part that's ok (left for the next iteration)
            yield None, (pred_start, real_start - MAX_OVERPREDICTION - 1)
            pred_start = real_start - MAX_OVERPREDICTION
        elif pred_end - real_end > MAX_OVERPREDICTION:
            # the prediction ends too late, so we split it into two parts:
            # - the part that's ok (yielded now)
            # - the part of the prediction that's too late (left for the next iteration)
            yield (real_start, real_end), (pred_start, real_end+MAX_OVERPREDICTION)
            pred_start = real_end + MAX_OVERPREDICTION + 1
            update_real = True
        else:
            # otherwise, the prediction ends within the maximum allowed time
            yield (real_start, real_end), (pred_start, pred_end)
            # we advance only the crisis (real or predicted) which ends earlier,
            # because the later crisis might still have other intersections
            if pred_end < real_end and pred_it.has_next() and pred_it.peek()[0] < real_end:
                # in this case, the next predicted crisis will intersect with the current real crisis,
                # so we don't update the real crisis
                update_pred = True
            else:
                # otherwise, we update both
                # note that a predicted crisis can only intersect a single real crisis
                update_real = update_pred = True


def compute_metrics(real_crises: List[dict],
                    pred_crises: List[dict],
                    crises_intersections: List[dict],
                    session_durations: Dict[Tuple[str, str], int],
                    model: str) -> Tuple[List[dict], dict]:
    """Computes a model's confusion matrix metrics

    Returns
    -------
    A tuple (tagged_sessions, metrics) where:
     - tagged_sessions is a list of seizures, classified as either
       true positive (tp), true negative (tn), false positive (fp) or
       false negative (fn). A seizure is classified as a true positive
       if at least 75% of it was predicted by the model.
     - A dict of tp/tn/fp/fn metrics computed over individual predictions
       and crises_tp/crises_tn/crises_fp/crises_fn metrics computed over
       each seizure (as explained above).
    """
    tagged_crises = []
    processed_pred_ids = set()
    crises_metrics = {metric: 0 for metric in ('crises_fp', 'crises_tp', 'crises_fn', 'crises_tn')}

    real_to_pred = {real_id: [] for real_id in range(len(real_crises))}
    for intersection in crises_intersections:
        if pred_crises[intersection['pred_id']]['model'] == model:
            real_to_pred[intersection['real_id']].append(intersection['pred_id'])

    for real_id, pred_ids in real_to_pred.items():
        real_start = real_crises[real_id]['start']
        real_end = real_crises[real_id]['end']
        real_length = real_end - real_start + 1

        # Compute the intersection length
        intersection_length = 0
        time_start = real_crises[real_id]['time_start']
        time_end = real_crises[real_id]['time_end']
        for pred_id in pred_ids[::]:
            pred_start = pred_crises[pred_id]['start']
            pred_end = pred_crises[pred_id]['end']
            assert real_start - pred_start <= MAX_OVERPREDICTION
            assert pred_end - real_end <= MAX_OVERPREDICTION

            intersection_length += min(pred_end, real_end) - max(pred_start, real_start) + 1
            processed_pred_ids.add(pred_id)
            time_start = min(time_start, pred_crises[pred_id]['time_start'])
            time_end = max(time_end, pred_crises[pred_id]['time_end'])

        # Detect false positive
        overlap = intersection_length / real_length
        classification = 'tp' if overlap > MIN_PREDICTION_OVERLAP_PERCENT else 'fn'
        crises_metrics['crises_' + classification] += 1
        tagged_crises.append({
            'real_id': real_id,
            'pred_id': None,
            'time_start': time_start,
            'time_end': time_end,
            'patient_id': real_crises[real_id]['patient_id'],
            'session_id': real_crises[real_id]['session_id'],
            'overlap': overlap,
            'classification': classification,
        })

    crises_per_session = {}
    for pred_id in range(len(pred_crises)):
        if pred_id in processed_pred_ids or pred_crises[pred_id]['model'] != model:
            continue
        tagged_crises.append({
            'real_id': None,
            'pred_id': pred_id,
            'start': pred_crises[pred_id]['start'],
            'end': pred_crises[pred_id]['end'],
            'time_start': pred_crises[pred_id]['time_start'],
            'time_end': pred_crises[pred_id]['time_end'],
            'patient_id': pred_crises[pred_id]['patient_id'],
            'session_id': pred_crises[pred_id]['session_id'],
            'model': model,
            'overlap': 0.0,
            'classification': 'fp'
        })
        key = pred_crises[pred_id]['patient_id'], pred_crises[pred_id]['session_id']
        crises_per_session.setdefault(key, []).append(tagged_crises[-1])
        crises_metrics['crises_fp'] += 1

    for (patient_id, session_id), duration in session_durations.items():
        relevant_crises = crises_per_session.get((patient_id, session_id), [])
        relevant_crises.sort(key=lambda c: c['start'])
        last_end = -1
        for c in relevant_crises:
            if last_end + 1 < c['start']:
                crises_metrics['crises_tn'] += 1
            last_end = c['end']
        if last_end != duration:
            crises_metrics['crises_tn'] += 1

    return tagged_crises, crises_metrics


def get_ranges(iterable: Iterable, target_val) -> Iterable[Tuple[int, int]]:
    """Returns the ranges where `iterable` has value `target_val`

    The ranges are given in the form of tuples (start_index, end_index).
    The indices are inclusive.
    """
    start = None
    for i, val in enumerate(iterable):
        if start is None and val == target_val:
            start = i
        elif start is not None and val != target_val:
            yield (start, i-1)
            start = None
    if start is not None:
        yield (start, i)


def parse_compute_metrics_and_crises_args(
        args_to_parse: List[str]) -> argparse.Namespace:
    """
    Parse arguments for adaptable input.

    parameters
    ----------
    args_to_parse : List[str]
        List of the element to parse. Should be sys.argv[1:] if args are
        inputed via CLI

    returns
    -------
    args : argparse.Namespace
        Parsed arguments
    """
    parser = argparse.ArgumentParser(description='CLI parameter input')
    parser.add_argument('--cons-folder',
                        dest='cons_folder',
                        help="The path of the folder which contains prediction (csv) files")
    parser.add_argument('--output-folder',
                        dest='output_folder',
                        help="The path of the folder to which the metrics and seizure information should be saved")
    args = parser.parse_args(args_to_parse)

    return args


if __name__ == '__main__':
    args = parse_compute_metrics_and_crises_args(sys.argv[1:])
    args_dict = convert_args_to_dict(args)
    compute_metrics_and_crises(**args_dict)
